# 基础

1. 与其构建更快，更复杂的单处理器，不如在单个芯片上仿制多个相对简单的处理器，这样的集成电路称为多核处理器。
2. 不存在一个翻译器，可以将串行程序，直接翻译成并行程序。
3. 串行程序的高效并行实现可能不是通过挖掘其中每一步的高效并行实现来获得的，最好的并行化实现可能是通过一步步回溯，然后发现一个全新的算法来获得的。因为某些串行算法不适合并行化。
4. 归约是并行编程中比较重要的一个环节，将任务分给多个线程或进程后，最终可能需要将他们各自计算的结果，统一到一起，这个过程称为归约，Reduce。常见的方法有：
   1. 每个线程都计算完之后，将他们的结果都发送给某个线程，一般是0号线程，由他来进行统一归约，这会导致0号线程所在的核承担更多的工作。这是原来串行程序中经常使用的方法。
   2. 各个线程两两结对，1号线程将自己的值发送给0号，3号线程将自己的值发送给2号，以此类推。一轮进行完成之后，只有0，2，4，6等号线程上还有数据，同样应用，让2号线程把数据发送给0号线程，6号线程把数据发送给4号线程。这样对于N个线程，一共需要$log_2^N$​轮。这种方法在并行程序中经常使用。
   3. <img src="并行计算.assets/image-20240802230052211.png" alt="image-20240802230052211" style="zoom:50%;" />

5. 并行编程常用的两种方法：
   1. 任务并行：将需要解决的任务分配到各个核上执行。
   2. 数据并行：将要解决问题所需要处理的数据分配给各个核，每个核在分配到的数据上执行类似的操作。

6. 例如：5个人给100张卷子打分，每张卷子有10道题目。任务并行是指：每个人分别对100张卷子的2道题目进行评分，此时每个人执行的指令是不同的。数据并行是指：每个人分别对20张卷子的所有题目打分，此时每个人执行的指令是相同的。
7. 另一个例子是，计算100个数的函数值，然后对结果求总和。在这个方法中，可以分为2大部分：
   1. 每个线程都去计算自己那部分数据的函数值，并计算其部分和，这可以看作是数据并行，因为每个线程都是在部分数据上执行类似的操作。
   2. 0号线程执行接收数据的命令，并在接收到后，对其求和，其余线程执行发送数据的命令。这可以看作是任务并行，因为这里一共有2个任务，而每个线程的任务不一样。

8. 多个线程之间是需要协调的，这个过程包括：
   1. 通信，例如将自己的部分和发送给0号线程。
   2. 负载均衡，希望给每个线程分配大致相同的工作量，避免出现一核有难，八核围观的情况。注意工作量相同不一定表示数据量相同，因为某些数据可能比另一些数据要好处理。
   3. 同步，有时希望线程之间按照某种特定的顺序来进行工作，例如在处理输入的线程还没有将数据初始化完成前，其他要处理数据的工作线程不能来取数据。

9. 目前，功能最强大的并行程序是通过显式的并行结构来编写的，这类程序通常非常复杂。也有一些跟高级的语言来简化这一步骤，但是会牺牲灵活性或性能。
10. MPI和Pthreads是C语言的扩展库，可以在C程序中使用扩展的类型定义，函数或宏。OpenMP包含了一个扩展库以及对C编译器的部分修改。
11. 主要关注的并行系统分为2种：
    1. 共享内存系统（下图a）：理论上，每个核能够读写内存的所有区域，因此可以通过检测和更新共享内存中的数据来协调各个核。OpenMP和Pthreads是这种，之所以有两种，是因为OpenMP是对C语言更高层次的扩展，Pthreads提供了一些在OpenMP中不可用的协调构造。
    2. 分布式系统（下图b）：每个核都拥有自己的内存，核之间的通信是显式的，必须使用类似于在网络中发送消息的机制。MPI是这种。
    3. ![image-20240802234611293](并行计算.assets/image-20240802234611293.png)

12. 容易混淆的术语如下：
    1. 并发（cocurrent），一个程序的多个任务在同一个时间段内可以同时执行。这里强调是时间段，并非同一时刻。
    2. 并行（parallel），一个程序通过多个任务紧密协作来解决某个问题。
    3. 分布式（distribute），一个程序需要与其他程序写作来解决某个问题。

13. 并行程序和分布式程序都是并发的，但是某些程序如多任务操作系统也是并发的，因为即使它运行在单核机器上，多个任务也能在同一时间段内同时执行。
14. 并行程序和分布式程序之间没有明显的界限。并行程序往往同时在多个核上执行多个任务，这些核在物理上紧密靠近或者共享内存或者通过高速网络相连。分布式程序往往更加松耦合，任务是在多个计算机上执行，计算机之间距离较远，且任务是由独立创建的程序来完成的，一般的大型网站服务器都是分布式的，可能在各地都有镜像。

# 并行硬件和软件

1. 经典的冯诺依曼结构包括主存，中央处理单元（CPU）以及主存和CPU的互连结构。指令和数据通过互连结构来传输，通常是总线。随着CPU速度的提高，主存和CPU的分离逐渐成为冯诺依曼结构的瓶颈，CPU执行指令的速度已经远超过CPU从主存中取指令的速度了。

2. 为了改进这一瓶颈，出现了三种技术：

   1. 缓存，Cache，CPU的缓存是一组相比内存，CPU能够更快速访问的区域，一般和CPU在同一块的芯片上。
   2. 虚拟存储器（虚拟内存），VMEM，如果一个程序占用的内存较多，那么内存中可以同时存放的程序就比较少，这不利于系统的多任务处理。使用虚拟内存，主存可以作为外存的cache，主存中只存放当前和加下来会用到的部分，暂时不用的部分会放在外寸的交换空间中。
   3. 低层次并行，又称为指令级并行（Instruction-Level parallelism），通过让多个处理器部件或者功能单元同时执行指令来提高处理器的性能，主要有：流水线和多发射。


## 缓存

1. 通常情况下，程序接下来可能会用到的指令和数据与最近访问过的指令和数据在物理上是邻近存放的，也就是说执行完一条指令后，很可能执行其后的另一条指令，访问一个内存区域后，很可能访问其后的另一个内存区域。

2. 局部性原理：程序访问完一个存储区域后，往往会很快（时间局部性）访问邻近（空间局部性）的区域。为了运用局部性原理，可以在程序读取一个内存单元时，不只是将该单元放置到缓存中，而是将包含他的一整块内存（称为高速缓存行）都放入到缓存中。一个典型的高速缓存行，可以存储8~16个单个内存区域的信息。

3. CPU的Cache在其内部也是分层的，L1的cache距离CPU最近，可以认为它是低级的L2 cache的cache。所以如果一个变量存储在L1 cache中，那么它必然存储在L2及以下的所有cache及主存中。有些系统中，高层的cache不会存储在低层的cache中，而是会直接和主存关联。

4. 当CPU要读写内存单元时，它会沿着cache逐层向下查找，找到即称为cache命中，否则为缺失。命中和缺失是相对于某一层来说的，有可能在L1缺失，而在L2命中。

5. 当发生cache的读缺失时，CPU会从主存中读出包含所需信息的整个高速缓存行。当向cache中写入数据时，cache中的值就会和主存中的值不一致，有两种方法来解决：

   1. 写直达，write-through，高速缓存行会立即将更新同步到主存。
   2. 写回，write-back，CPU将发生数据更新的整个高速缓存行标记为脏dirty，当后续要有新的数据存入高速缓存行时，将此脏的高速缓存行写入内存中。

6. 一个cache一般由多个高速缓存行组成，当需要将一个高速缓存行大小的数据存入到cache时，将其放到哪个位置存在多种方式：

   1. 全相联（fully associative），数据可以存放在cache的任何一个高速缓存行。
   2. 直接映射（directed mapped），数据在cache中有唯一对应的高速缓存行。
   3. n路组相联（n-way set associated），是前两种的中间方案，数据可以存放在cache中的n个不同高速缓存行中的任意一个。

7. 例如，主存有16行，编号为0-15，cache有4行，编号为0-3。

8. <img src="并行计算.assets/image-20240803010420003.png" alt="image-20240803010420003" style="zoom: 67%;" />

9. 当内存中多于一行能被映射到cache的多个不同位置时（全相联或n路组相联），需要决定替换或者驱逐cache的哪一行。例如在2路组相连中，主存的0号已经存在于cache的0号，主存的2号已经存在于cache的1号，那么此时如果要将主存的4号读入cache，应该将二者中哪一个cache行换出呢？。一般使用最近最少使用（least recent used），cache会记录各个高速缓存行被访问的次数，最近访问次数最少的高速缓存行会被替换掉。

10. 非常重要的一点是，CPU的cache是由硬件来完全控制的，程序员不能人为控制，只能利用局部性原理来间接地利用cache带来的高效。例如C语言以行主序（一行一行存储）来存储二维数组，因此存在如下两个矩阵和向量的乘法的算法：

    ```c
    double A[2][3], x[3], y[2];
    //初始化A,x，给y赋值为0，计算y += A*x。
    //第一种，每个内层循环，都是计算A的一行和x做内积，然后叠加到y的该行上去。
    for (i = 0; i < m; i++)
        for (j = 0; j < n; j++) //内层循环中，j变化，也就是列变化，这些单元在内存中是连续的，因此一次cache的读取可以存入多个A的计算单元。
            y[i] += A[i][j]*x[j];
    //第二种，只是交换两侧循环，其余什么也不修改
    for (j = 0; j < n; j++)
        for (i = 0; i < m; i++)
            y[i] += A[i][j]*x[j];
    ```

11. 实例化上面的例子：

    ```c
    A = {1,2,3,
         4,5,6}
    x = {2,1,3}
    y = {0,0,0}
    //第一种情况，按照矩阵-向量乘法的定义进行
    // y[0]+=1*2;y[0]+=2*1;y[0]+=3*3
    // y[1]+=4*2;y[1]+=5*1;y[1]+=6*3
    //第二种情况，将矩阵-向量乘法看作是对矩阵的列向量组进行线性组合，系数为x向量的元素。可以发现这种情况下，矩阵会按列访问，这和矩阵的存储方式不一样，因此无法利用局部性。
    // y[0]+=1*2;y[1]+=4*2;
    // y[0]+=2*1;y[1]+=5*1;
    // y[0]+=3*3;y[1]+=6*3;
    ```


## 虚拟内存

1. 虚拟内存是由系统硬件和操作系统一起控制的。

2. 和cache的高速缓存行类似，虚拟内存也是按照块来组织的，基本的单位称为页，因为外存比主存要慢很多，因此页一般比较大，从4-16kB不等。

3. 如果编译时指令中的内存地址都是物理地址，这会限制程序被载入的位置，也不利于多任务并行。因此编译时指令的地址都是虚拟地址。程序运行时，CPU的MMU部件会将虚拟地址转化为物理地址，转换的关系存储在内存的页表中，页表可以有多级。

4. 页表会增加CPU访问内存的时间，每次查询页表都会浪费时间，因此出现了TLB（Translation Lookaside Buffer），转译后备缓冲区，快表，TLB就是页表的Cache，属于MMU的一部分，其中存储了当前最可能被访问到的页表项，其内容是部分页表项的一个副本。

5. 和cache的两种写入策略不一样的是，虚拟内存一般只采用写回策略，因为外存的速度太慢了。


## 指令级并行

1. 流水线是指将功能单元分阶段安排，例如一个浮点数加法操作，可以分解为取操作数，比较指数，对其中一个操作数移位，相加，规格化结果，舍入结果，存储结果这7部分组成。如果每个操作花费1ns，则1次加法操作花费7ns，假设有1000对浮点数要相加，不使用流水线的话，需要7000ns，如果使用流水线，在第1个加法操作执行其第2阶段时，第2个加法操作可以执行器第1阶段，因为此时取操作数的部件是空闲的。此时1000对浮点数的加法，将会划分7+1000-1=1006ns。

2. 也就是说流水线允许每条执令处在流水线的不同阶段。总的来说，k级流水线，不可能达到k倍的性能提高。

3. 多发射是通过赋值功能单元来同时执行程序中的不同指令，例如要对两个向量x，y求加法，那么当第一个加法器计算x[0]+y[0]时，第二个加法器可以同时计算x[1]+y[1]。

4. 如果功能单元时在编译时调用的，则称为静态多发射系统，如果是在运行时调度的，则成为动态多发射系统。一个支持动态多发射的处理器称为超标量。

5. 为了能够进行多发射，系统必须能找出能够同时执行的指令，其中最重要的技术是预测，编译器或处理器对一条指令进行猜测，然后在猜测的基础上执行代码，例如：

   ```c
   z = x + y;
   if(z > 0){
       //系统可能预测z的结果为正，选择在执行第1行的同时，也执行这个行。
   }else{
       
   }
   ```

6. 预测执行允许错误的发生，出错后需要回退，然后执行正确的指令。

7. 如果预测工作由编译器来做，那么它通常会在代码中嵌入测试语句来验证预测的正确性，如果预测错误，就会执行修正操作。如果由硬件来进行预测，处理器一般会将预测执行的及格过缓存在一个缓冲器中，如果预测正确，则缓冲器中的内容会传递给寄存器或内存，反之会被丢弃。

8. 编译器优化技术能够对指令进行重新排序，这一操作会对共享内存编程产生重大影响。

9. 指令级并行是很难利用的，因为程序中有许多部分之间存在依赖关系。

10. 线程级并行，尝试同时执行不同的线程来提供并行性。和指令级并行来比，它是粗粒度的，前者是指令流，后者是单条指令。

    1. 细粒度多线程，处理器在每条指令执行完后都会切换线程，从而跳过被阻塞的线程，它能够避免阻塞造成的时间浪费，但是缺点是切换线程的开销比较大。

    2. 粗粒度多线程，只会在需要等待较长时间完成操作而被阻塞的指令时切换线程，优点是不需要经常切换，缺点是处理器还是有可能在短阻塞时空闲，线程的切换也会导致延迟。

    3. 同步多线程（SMT），是细粒度多线程的变种，通过允许多个线程同时使用多个功能单元来利用超标量处理器的性能。可以指定优先（有多条指令就绪的）进程，这样可以在一定程度上减轻线程减速的问题。

11. Flynn分类法，对计算机体系结构分类：

    1. SISD，典型的冯诺依曼体系就是这种结构，一次执行一条指令，一次存取一个数据项。

    2. SIMD，可以认为有一个控制单元和多个ALU，一条指令从控制单元广播到多个ALU，每个ALU。

12. 

13. 

14. 

15. 

16. 

17. 

18. 

19. 

20. 

21. 

22. 

23. 

24. 

25. 

26. 多处理器的加速比=使用单个处理器计算所消耗的时间/使用多个处理器计算所消耗的时间。

27. 多个处理器各自计算的结果需要汇总或相互交流，这限制了达成完美的加速比。

28. 任务分配不均匀也会导致某些处理器等待其他处理器的情况。静态负载均衡是在开始前分配，动态负载均衡在运行时也可以调配负载，不过会产生开销。

29. 如果将任务分配的过于细小，则CPU之间交流占用时间会比实际计算的时间要长，也会限制加速比。

30. Flop：Floating point operation，通常指双精度浮点计算。随着深度学习的引入，16，8位浮点数越来越常见。

31. 超级计算机IBM Summit，理论峰值为200PFlop/s，实测峰值为148.6PFlop/s。性能是通过Linpack库测出来，包括通过LU分解法求解大规模非稀疏，系数随机的线性方程组。Linpack是比较理想化的，实测峰值可以达到理论峰值的70%，实际问题中，大约10%不到。

32. 早期的超级计算机就是一台超级大的计算机，多个处理器通过共享内存相互协作，现代的超级计算机都是很多台计算机，他们之间通过超高速网络相连接，多个计算节点组成的集群，MPI也是这时期发展来的。再往后就变成异构了，CPU和GPU都存在。

33. Seymour Roger Cray是超级计算机之父，他说过：任何人都可以构建一个快的处理器，而构建一个快的系统是困难的。系统涉及处理器，软件，算法等。

34. 第一台超级计算机CDC6600。比当时最快的计算机快10倍，浮点性能为3MFlop/s。

35. 国内超算的派系：曙光（中科院计算所），银河/天河（国防科大），神威（无锡江南计算所）

36. 1996年中国宣布禁止核试验后，对于超算的需求更大了。

37. 目前x86指令集在超算领域已经一家独大了。

38. 并行和并发的区别：
    1. 并行，Parallelism，指的是多个子任务同时运行，控制流的状态。需要考虑内存一致性。
    2. 并发，Concurrency，指的是控制流的一种同时运行的属性，需要考虑信号处理。

39. 任务并行和数据并行的区别，用于切分的对象不同，前者是任务，后者是数据。有时并没有严格的界限。

40. 两类并行编程模式，根据对内存的访问方式来区分：
    1. 共享内存并行编程，数据天然就是共享的，不过对数据的访问需要手动同步。Pthread就是这种，它是一种偏底层的方式。OpenMP也是，简化了很多，借助于编译器来节省很多代码的编写，自动生成。
    2. 分布式内存并行编程，相对更难点，数据共享和通讯都需要手动完成。通过消息传递方式，MPI就是这种。

41. 并行程序的调试比较麻烦，工具可能不一定好用，多用printf，人工调试。

42. 提高处理器的数量，只能加速程序中并行的部分，不可并行的部分不受影响。

43. 并行计算中的开销有：开启新线程或进程的开销，交换共享数据的开销，同步访问中等待的开销，冗余计算的开销。

44. Flynn分类法，根据指令是如何处理数据，可以分为：
    1. SISD，单指令单数据模型，可以通过流水线来提高性能。
    2. SIMD，单指令多数据模型，也称为向量机，SSE，AVX指令都是这种。
    3. MISD
    4. MIMD

45. 

46. 

# OpenMP

1. 除了使用线程库之外，还可以在语言层面就加入并发，例如go语言。而OpenMP（Open Multi-Processing）就是介于POSIX和并发语言之间的，它通过一些编译器指令来为一些编程语言添加并发功能，所以它对编译器版本有要求，gcc 4.2之后的版本才支持OpenMP。和Pthread一样，OpenMP也是共享内存模式的。
2. 虽然OpenMP的英文是Open Multi-Processing，即多进程，但是它往往是多线程的形式出现。
3. 优点：

   1. 和Pthread相比，它是跨平台的，只需要重新编译链接即可，使用也比较简单。

   2. 当不使用-fopenmp时，`#pragma omp`指令就是注释，不发挥任何作用。

   3. 不需要修改原始代码就可以使之并行，可以减少无意中引入bug的可能。

   4. 粗粒度和细粒度并行都是可能的。

   5. 

4. 目前由OpenMP Architecture Review Board (OpenMP ARB)负责维护。

5. 使用并行编程的混合模型构建的应用程序可以在使用OpenMP和消息传递接口（MPI）的计算机集群上运行，这样OpenMP用于（多核）节点内的并行性，而MPI用于节点之间的并行性。

6. OpenMP是多线程模式，从主线程（ID为0）上创建出多个子线程，最后都join回主线程。

7. 使用OpenMP线程，需要在gcc编译和链接时加入-fopenmp选项。makefile中使用`CFLAGS+=-fopenmp`和`LDFLAGS+=-fopenmp`。安装编译器时，就会自动安装openmp的库和头文件。

8. 功能包括：基本的并行区、并行for循环，高级的线程数控制和任务调度策略。

9. 例子：

   ```c
   #include <stdio.h>
   #include <stdlib.h>
   int main()
   {
   // 可以不加大括号，此时只对下面的第一条语句有效。
   // 会自动探测当前可用的CPU核心个数，会自动为每个核心开启一个线程。
   #pragma omp parallel
     { //需要注意的是，这个{必须单独出现一样，不能放在上一行的末尾，否则会提示括号不闭合
       printf("hello\n");
       printf("world\n");
     }
     return 0;
   }
   ```

10. 编译链接并运行上述例子：

    ```shell
    zj@zj-hit:~/test/C$ gcc main.c -o main -fopenmp -Wall # 如果不加-fopenmp，一般不会提示任何问题，除非开启了所有警告，例如-Wall，此时会提示warning: ignoring ‘#pragma omp parallel’ [-Wunknown-pragmas]
    zj@zj-hit:~/test/C$ ./main #当前系统有3个核心，所以会产生3个线程来执行并行区域的代码，因此会输出3遍。
    hello
    world
    hello
    world
    hello
    world
    ```

11. 实际上多个线程同时调用printf时，可能会出现竞争情况，一般情况下printf的实现不是线程安全的，而C++11的`std::cout`默认是线程安全的。

12. 如果没有OpenMP的函数可以不包含`<omp.h>`这个头文件。

13. 分区域：

    ```c
    #include <stdio.h>
    #include <stdlib.h>
    #include <omp.h>
    int main()
    {
    #pragma omp parallel sections //声明以section来运行，会将每个section分配给一个线程来运行
      {
    #pragma omp section //标记一个section，可以加括号，这里只有一行，所以省略了
        printf("[%d] hello\n", omp_get_thread_num()); //这里获取线程ID
    #pragma omp section //标记另一个section
        printf("[%d] world\n", omp_get_thread_num());
      }
      exit(0);
    }
    ```

14. 上述例子可能的输出结果：

    ```shell
    #可能的输出结果如下，可以发现不一定按照section声明的顺序分配线程，同时也不一定按照声明的顺序或者线程编号的顺序来决定运行的先后顺序。
    zj@zj-hit:~/test/C$ ./main 
    [1] hello 
    [0] world
    zj@zj-hit:~/test/C$ ./main #最理想的情况
    [0] hello
    [1] world
    zj@zj-hit:~/test/C$ ./main 
    [0] world
    [1] hello
    ```

15. OpenMP最常用的功能之一是并行化for循环：

    ```cpp
    #include <omp.h>
    #include <iostream>
    
    int main()
    {
      const int size = 10;
      int array[size];
      // 初始化数组
      for (int i = 0; i < size; ++i)
      {
        array[i] = i;
      }
    // 将for循环并行化
    #pragma omp parallel for // 只会处理下一行的for循环
      for (int i = 0; i < size; ++i)
      {
        int thread_id = omp_get_thread_num();
        std::cout << "Hello from thread " << thread_id << "  i =  " << i << std::endl;
        array[i] = array[i] * array[i]; //假设核心有3个，将循环的次数10/3=3...1，则0号线程会被分配去执行前i=0-3的4次循环，1号线程被分配去执行i=4-6的3次循环，2号线程被分配去执行i=7-9的3次循环。但是每个线程的执行先后顺序不一定。
      }
      // 输出结果
      for (int i = 0; i < size; ++i)
      {
        std::cout << array[i] << " ";
      }
      std::cout << std::endl;
      return 0;
    }
    ```

16. 可以通过 `omp_set_num_threads` 函数或者在编译指示中指定线程数，这种方式可以精细控制线程的使用，避免资源过度消耗：

    ```c
    #pragma omp parallel for num_threads(4) //仅使用4个线程来并行
    for(int i = 0; i < size; ++i) {
        array[i] = array[i] * array[i];
    }
    ```

17. OpenMP提供了多种调度策略，用于分配任务给不同的线程。常见的有 `static`、`dynamic` 和 `guided` 等：

    ```c
    #pragma omp parallel for schedule(static, 2) //设置调度策略为static，附带一个参数2，表示每个线程处理2个连续的迭代。这种策略适用于负载均衡的任务。
    ```

18. 在多线程编程中，数据竞争是一个常见问题。OpenMP提供了 `critical` 和 `atomic` 指示符，用于保护共享资源：

    ```cpp
    #include <omp.h>
    #include <iostream>
    int main()
    {
      int sum = 0;
    #pragma omp parallel for
      for (int i = 0; i < 200; ++i)
      {
    #pragma omp critical //设置了临界区后，不同线程执行此代码时都会等待获取锁（这里不用显式处理锁的细节），同一时刻只能有一个线程执行此行。默认只对下面的一行有效，也可以加括号。
        sum += i;
      }
      std::cout << "Sum: " << sum << std::endl;
      return 0;
    }
    ```

19. 如果没有设置临界区，则会造成如下运行结果，因为sum += i是由读取，做加法，写回三步构成的，任意一步之间都有可能被其他线程打断：

    ```shell
    zj@zj-hit:~/test/C$ ./main 
    Sum: 17699
    zj@zj-hit:~/test/C$ ./main 
    Sum: 17689
    zj@zj-hit:~/test/C$ ./main 
    Sum: 17689
    zj@zj-hit:~/test/C$ ./main 
    Sum: 17690
    zj@zj-hit:~/test/C$ ./main
    # 正确结果应为
    zj@zj-hit:~/test/C$ ./main 
    Sum: 19900
    ```

20. 一个矩阵乘法的例子，这是一个计算密集型任务，非常适合使用OpenMP进行并行化处理：

    ```cpp
    #include <iostream>
    #include <vector>
    #include <omp.h>
    int main()
    {
      const int N = 1000;
      std::vector<std::vector<int>> A(N, std::vector<int>(N, 1)); // 矩阵的所有元素都是1
      std::vector<std::vector<int>> B(N, std::vector<int>(N, 2));
      std::vector<std::vector<int>> C(N, std::vector<int>(N, 0));
    
    #pragma omp parallel for collapse(2) // 将外层两个循环一起并行化，这样可以更好地利用多线程的优势。
      for (int i = 0; i < N; ++i)
      {
        for (int j = 0; j < N; ++j)
        {
          for (int k = 0; k < N; ++k)
          {
            C[i][j] += A[i][k] * B[k][j];
          }
        }
      }
      std::cout << "Matrix multiplication completed." << std::endl;
      return 0;
    }
    ```

21. 运行时间差异：

    ```shell
    #使用OpenMP，墙上时间大约4秒多
    real    0m4.827s
    user    0m14.262s #这里之所以user比real多，是因为它会重复计算多个核心的时间
    sys     0m0.042s
    #不使用OpenMP，墙上时间大约13秒多，明显加速了。
    real    0m13.454s
    user    0m13.444s
    sys     0m0.005s
    ```

22. 

23. 
24. 

# MPI

1. 在1990年之前，各种硬件架构平台上有各自的并行编程接口。消息传递接口（MPI）是一种标准化的、可移植的消息传递标准，被设计在并行计算架构上运行。它只是标准，并没有规定实现。它适用于分布式内存系统上运行的进程之间通信。
2. MPI并没有被IEEE或ISO等机构收录，但是已经是事实上的标准了。MPI标准是由MPI论坛搞出来的，1994年和1996年分别发布了MPI-1和MPI-2标准。MPI-2只是对MPI-1的增加，而非替代。目前最新的是4.1。
3. MPI可以看作是一个中间件，连接上层的应用程序，和下层的操作系统。
4. MPI是给科学家和工程师使用的，而非专业的计算机科学家，他们不必了解网络是如何运行的。它简化了网络的使用，对共享内存和socket进行了抽象。只需要使用MPI_Send和MPI_Recv即可，底层的网络会自动配置。
5. 标准也在不断更新，一般都是x.0，然后x.1，在之后就是x+1.0了。
6. MPI实现（MPICH，Intel MPI，MS MPI，HP MPI，Open MPI等）的ABI大致分为MPICH和Open MPI衍生物，来自一个系列的库可以作为同一系列库的直接替换，但跨系列的直接替换是不可以的。
7. MPICH，以前称为MPICH2，是MPI的免费、可移植的实现，标准的覆盖最全，可以认为是参考实现，不支持一些新的硬件，例如InfiniBand。它的衍生品例如Intel MPI是支持新的硬件和协议的。
8. Open MPI的进程管理是比MPICH要好的。
9. MPICH包含了很多衍生的库，例如Intel MPI，MS-MPI，Open MPI则只有它自己。
10. MPI是SPMD（单程序，多数据），程序被执行多次，但是处理的数据不一样，只是处理数据的逻辑是一样的。

## Open MPI

1. Open MPI（注意是2个单词）也被称为OMPI。

2. 查看Open MPI的版本：

   1. 通过`/usr/lib/x86_64-linux-gnu/Open MPI/include/Open MPI/ompi/version.h`文件查看Open MPI的版本。

   2. 通过包管理器查看，这个只适用于通过包管理器直接安装二进制程序的情况，`apt list --installed |grep Open MPI`。

   3. 通过ompi_info程序来查看：

      ```shell
      zj@zj-hit:~/test/C$ ompi_info
                       Package: Debian OpenMPI
                      Open MPI: 4.1.2
        Open MPI repo revision: v4.1.2
         Open MPI release date: Nov 24, 2021
                      Open RTE: 4.1.2
        Open RTE repo revision: v4.1.2
         Open RTE release date: Nov 24, 2021
                          OPAL: 4.1.2
            OPAL repo revision: v4.1.2
             OPAL release date: Nov 24, 2021
                       MPI API: 3.1.0 #实现的MPI标准
                  Ident string: 4.1.2
                        Prefix: /usr
       Configured architecture: x86_64-pc-linux-gnu
                Configure host: hostname
                 Configured by: username
                 Configured on: Fri Mar  4 12:35:12 UTC 2022
                Configure host: hostname
      ...
      #或者使用ompi_info -V 来获取简洁的信息。
      #这个命令还可以查看已安装的Open MPI插件列表，并查询它们支持的MCA参数
      ```

3. Open MPI社区本身并不分发Open MPI的二进制包，但许多下游的打包者（例如Linux发行版）确实分发了。

4. Open MPI由三个软件层组成：OPAL（开放可移植访问层）、ORTE（开放运行时环境）和OMPI（开放MPI）。每一层都有包装编译器；每一层的包装器只链接与该层相关的库。具体来说，每一层都提供以下包装编译器：

   ```shell
   OPAL # opalcc和opalc++
   ORTE # ortecc和ortec++
   OMPI # mpicc、mpic++、mpicxx、mpiCC（仅适用于具有区分大小写文件系统的系统）和mpifort（及其遗留/弃用的名称mpif77和mpif90）。
   ```

5. mpicc（以及类似的mpic++、mpif90等）是一个程序，它封装了现有的编译器，在编译使用MPI的代码时设置必要的命令行标志。通常，它会添加一些标志，使代码能够根据MPI库进行编译和链接。如果使用默认的gcc，则会报错，`fatal error: mpi.h: 没有那个文件或目录`。

6. mpiexec是一个用于执行具有4个进程的示例程序的命令，每个进程在运行时都是程序的独立实例，并分配了等级（即数字ID）0、1、2和3。在编写程序时，并没有指定要并行的数量，因此如果使用`./main`来运行，则只会有一个进程。

7. 系统内的mpicc是一个符号链接，如果安装了多个MPI的实现时，它会指向其中一个。例如安装了Open MPI时，符号链接为：`/usr/bin/mpicc -> /etc/alternatives/mpi -> /usr/bin/mpicc.Open MPI`。而所有的Open MPI的程序都会指向`/usr/bin/opal_wrapper`。

8. Open MPI相关的头文件都在`/usr/lib/x86_64-linux-gnu/Open MPI/include`。

9. MPI提供的命令有：

   ```shell
   mpicc  #C编译器
   mpicxx，mpic++，mpiCC #都使用相同的选项调用相同的底层C++编译器。所有这些都是为了与其他MPI实现兼容而提供的。
   mpif77 #Fortran77编译器
   mpif90 #Fortran90编译器
   mpifort #支持最新的Fortran语言
   mpiexec #运行程序
   mpirun  #
   ```

10. 例子：

   ```c
   #include <assert.h>
   #include <stdio.h>
   #include <string.h>
   #include <mpi.h> //头文件
   int main(int argc, char **argv)
   {
     char buf[256];
     int my_rank, num_procs;
     MPI_Init(&argc, &argv);                    // 初始化，分配资源
     MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);   // 标识此进程
     MPI_Comm_size(MPI_COMM_WORLD, &num_procs); // 找出共有多少进程处于活动状态
     // 到目前为止，所有的程序做的动作都是一样的。下面使用rank=0的那个进程为其他所有进程发送消息。
     if (my_rank == 0)
     {
       int other_rank;
       printf("We have %i processes.\n", num_procs);
       // 向所有其他进程发送消息
       for (other_rank = 1; other_rank < num_procs; other_rank++)
       {
         sprintf(buf, "Hello %i!", other_rank);                       // 构造字符串
         MPI_Send(buf, 256, MPI_CHAR, other_rank, 0, MPI_COMM_WORLD); // 发送消息
       }
       // 接收来自其他进程的消息
       for (other_rank = 1; other_rank < num_procs; other_rank++)
       {
         MPI_Recv(buf, 256, MPI_CHAR, other_rank, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE); // 接收消息
         printf("%s\n", buf);                                                            // 打印到终端上
       }
     }
     else
     {
       // 对于其他进程，从0号进程接收消息，然后再发送回去。
       MPI_Recv(buf, 256, MPI_CHAR, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
       assert(memcmp(buf, "Hello ", 6) == 0);
       // 发送回去消息
       sprintf(buf, "Process %i reporting for duty.", my_rank);
       MPI_Send(buf, 256, MPI_CHAR, 0, 0, MPI_COMM_WORLD);
     }
     MPI_Finalize(); // 释放资源
     return 0;
   }
   ```

11. 编译链接运行的结果：

    ```shell
    zj@zj-hit:~/test/C$ mpicc main.c -o main
    zj@zj-hit:~/test/C$ mpiexec -n 3 ./main #运行4遍，也就是4个进程，my_rank分别为0，1，2，3。每个进程的my_rank不一样，但是num_procs一样。无法控制每个进程的执行顺序。
    We have 3 processes.
    Process 1 reporting for duty.
    Process 2 reporting for duty.
    # 对于只有一个3核的CPU的单台计算机，-n的参数最多为3。超出后会报错，
    --------------------------------------------------------------------------
    There are not enough slots available in the system to satisfy the 4
    slots that were requested by the application:
    
      ./main
    
    Either request fewer slots for your application, or make more slots
    available for use.
    ...
    ```

12. rank是一个通信器里进程的标号，必须是连续的，因为涉及到集合通信。

13. Communicator是通信器，定义了那些进程可以互相通信，使用的宏是MPI预定义的。MPI_COMM_WORLD是由所有进程组成的通信器。

14. MPI标准推荐使用名称mpiexec，尽管一些实现在名称mpirun下提供了类似的命令。都是同一个可执行文件的符号链接。

15. MPI的编程模型是SPMD（单程序多数据）。

16. 通信功能分为点对点（point-to-point）和集合（collectives）通信。

17. 发送和接收数据：

    ```c
    MPI_Send(void* buf, int count, MPI_Datatype datatype, int dest, int tag, MPI_Comm comm); //buf是要发送的内容的起始地址，发送count个datatype类型的数据。dest是接收者的rank，tag是消息的标记，应<MPI_TAG_UB。
    MPI_Recv(void* buf, int count, MPI_Datatype datatype, int source, int tag, MPI_Comm comm, MPI_Status *status);//buf是接收到的内容要存储的位置的起始地址，接收count个datatype类型的数据，source是发送者的rank，可以是MPI_ANY_SOURCE。status是输出参数，保存着此次接收消息的状态，如果不关心的话，可以设置为MPI_STATUS_IGNORE。
    ```

18. MPI预定义了很多数据类型，一般都是和C语言的基本数据类型对应的。

19. <img src="并行计算.assets/image-20240729231916702.png" alt="image-20240729231916702" style="zoom: 67%;" />

20. 许多MPI实现允许在同一MPI作业中启动多个不同的可执行文件。每个进程都有自己的排名、世界上进程的总数，以及它们之间通过点对点（发送/接收）通信或组间集体通信进行通信的能力。MPI只需提供一个SPMD风格的程序，其中包含MPI_COMM_WORLD、它自己的排名和世界的大小，就足以让算法决定要做什么。在更现实的情况下，I/O的管理比这个例子更仔细。MPI没有规定标准I/O（stdin、stdout、stderr）在给定系统上应该如何工作。它通常在rank-0进程上按预期工作，一些实现还捕获和汇集其他进程的输出。

21. MPI使用进程而不是处理器的概念。程序副本由MPI运行时映射到处理器。从这个意义上说，并行机可以映射到一个物理处理器，或映射到N个处理器，其中N是可用处理器的数量，甚至介于两者之间。为了获得最大的并行加速，使用了更多的物理处理器。此示例根据世界N的大小调整其行为，因此它也试图在不编译每个大小变化的情况下扩展到运行时配置，尽管运行时决策可能会根据可用的绝对并发量而有所不同。

# 超算

1. 超算一般是由多个节点组成的集群，每个节点可以看作是一个高级的个人计算机，节点之间通过高速的网络连接起来。一个节点可以有多个CPU，服务器的内存一般是ECC内存，具备纠错能力。

2. 天河2号是由16000个节点组成，每个节点有2个Xeon E5 2692处理器和3个Xeon Phi计算卡构成，计算卡也成为协处理器，Intel的这款计算卡具有最多61个处理器核心，每个核心拥有4个超线程，最多244个线程，超线程无法关闭。与计算卡竞争的是GPU。英特尔至强融核协处理器提供了类似于英特尔至强处理器编程环境的通用编程环境。多个英特尔至强融核协处理器可安装在单个主机系统中，这些协处理器可通过 PCIe 对等互连相互通信，不受主机的任何干扰。

3. 例如执行的是两个向量的加法：

   1. SISD（单指令单数据），每次对一对数据执行一次加法。
   2. SIMD（单指令多数据），每次对多对数据执行一次加法，将问题划分为连续的多段，每段逐个执行。这需要使用特殊的向量指令和较长的运算器来完成，该能够存储多对数据。
   3. SIMT（单指令多线程），将问题划分为多个不连续的段，每段逐个执行。例如奇数编号为一组，偶数编号为一组，或者根据编号除以某个数的余数来划分，余数相同的为一组。

4. SIMD在x86架构下，由AVX（Advanced Vector Extensions）/SSE指令集支持，优点是更告诉的计算方法，缺点是开发复杂度变高了，需要专用的CPU组件。

   ```c
   vectorAdd(float* a,float* b,float* c){ //SISD方式
       for (int i = 0; i < 8; i++){
           c[i]=a[i]+b[i];
       }
   }
   __m256 vectorAdd(__m256 a, __m256 b, __m256 c){ //SIMD方式，这里需要事先将8个float(正好256位)数据组装成AVX接受的数据类型__m256。
       return _mm256_add_ps(a,b); //通过一条CPU指令，执行8个float的相加
   }
   ```

5. SIMT的方式类似于GPU，GPU具有很多的运算器，使用一条指令可以调用多个运算器对其中的数据执行相同的运算，我们可以事先将数据放入这些运算其中。和SIMD不同的是，SIMT使用的是多个运算器，而非一个长长的运算器。将多个数据放入多个运算器的方式要比将它们放入一个长长的运算器中更简单，GPU的运算器之间是互相独立的，有更高的灵活性。SIMT无需开发者费力把数据凑成合适的矢量长度，并且允许每个线程有不同的分支。

6. SIMT的两个重要的库，CUDA（Compute Unified Device Architecture，统一计算架构）是由NVIDIA开发的，是该公司对GPGPU的正式名称。ROCm是AMD的对应产品。

7. 对于之前的8个float的加法，可以使用如下方法：

   ```c
   __global__ void addVector(float* a, float* b, float* c){
       int tid = blockIdx.x;
       if (tid<8){
           c[tid] = a[tid] + b[tid];
       }
   }
   ```

8. 可以使用OpenCL绕开Nvidia和AMD的库，它是一个为异构平台编写程序的框架。异构平台可以由CPU，GPU，DSP，FPGA或其他类型的处理器与硬件加速器组成。它是基于C/C++的语法，可以跨平台。和CUDA比，劣势是编译器不够成熟，运行速度不够快，库不够丰富。对于上面的例子使用OpenCL的实现如下，可以发现代码非常类似：

   ```c
   _kernel void vector_add(global const float* a, global const float* b, global const float* c){
       int gid = get_global_id(0);
       if(gid < 8){
           c[gid] = a[gid] + b[gid]
       }
   }
   ```

9. 多线程并行处理实际上是MIMD的方式，利用CPU的多核并行。OpenMP比Pthread更简洁，是一个支持跨平台的共享内存方式的多线程并发编程API。

10. 

11. 循环展开，可以降低分支预测的成本，例如将8次循环，每次做1次加法，替换为4次循环，每次做2次加法。现代CPU都是流水线架构，出现分支的地方可能会导致流水线无法被正常填充。CPU也提供分支预测的功能，来预测哪一个分支的概率更高。

12. 内存预取，在程序标记哪些内存需要提前载入到更高的缓存中，方便后续使用，这是比CPU的预测更准确的。

13. 

14. 

15. 

16. 

17. 

18. 

19. 

20. 

21. 

22. 

23. 

24. 

25. 

26. 

27. UMA（Uniform Memory Access）和NUMA（Non-Uniform Memory Access）是两种不同的内存访问架构。

28. UMA是一种对称多处理（SMP）系统的内存访问架构。在UMA系统中，所有处理器共享同一总线或交叉点，可以直接访问共享内存，这意味着无论哪个处理器访问内存，延迟都是相同的，UMA适用于小规模的多处理器系统。在NUMA系统中，每个处理器都有自己的本地内存和本地内存控制器，可以更快地访问本地内存。

29. NUMA是一种非对称多处理（NUMA）系统的内存访问架构，它指的是不同处理器对内存的访问延迟可能不同。

30. NUMA适用于大规模的多处理器系统，NUMA架构将物理内存分为一个个内存节点，内存节点和内存节点之间的物理地址有可能是不连续的，同一内存节点物理内存物理地址是连续的。如果一个处理器要访问其他处理器的本地内存，需要通过QPI总线去访问，这样延迟就会增加。

31. UMA架构可以理解为NUMA架构的一个节点，是NUMA架构的一种特殊情况。

32. 

33. 

34. 

35. 

36. 

37. 

38. 

39. 

40. 

41. 

42. 

43. 

44. 

45. 

46. 

47. 