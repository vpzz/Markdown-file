# Introduction

1. ![image-20200803172849205](机器学习.assets/image-20200803172849205.png)

2. 所谓的机器学习，就是让机器自己去找函数，满足人类需要的特定的功能。获得输入，产生输出。输入可以是声音信号，图片，棋谱，对话系统中接收到的内容等，输出对应为该声音信号对应的文字，图片对应的物体名称，下一步要落子的位置，将要回复的内容。

3. 回归（Regression），函数的输出是一个数值（scalar）。分类（Classification），函数的输出是对2个或多个选择的结果（选择题）。

4. 机器学习不只有回归和分类，还有生成（generation），即产生有结构的复杂的东西。实际是让机器做创造。

5. 监督学习 supervised learning，需要一批已经标注的输入输出对，labeled data。

6. 有了这些labeled data，机器就可以评估一个函数的好坏。每个函数都会有一个loss。即该函数对某些data输入的响应和data本身的标签不同，这就视为loss。从0到100%。loss越小，表示越接近理想的函数。

7. Alpha-Go就是用强化学习训练的，Reinforcement Learning。最开始是用现有的棋谱来做监督学习，然后再做强化学习。Alpha-Go zero是纯强化学习，不接受现有棋谱的训练，结果更优秀。

8. ![image-20200803174850477](机器学习.assets/image-20200803174850477.png)

9. 强化学习没有带标签的数据集，而是让机器自己做选择，然后看最终的结果是有益还是有弊。从而自己引导自己做出选择。

10. Unsupervised Learning 无监督学习，所有的data没有label。

11. 首先要为机器圈定所要寻找的函数的范围，然后其中寻找出最优的函数。函数的范围有传统的线性函数（分类和回归可以使用），神经网络（RNN，CNN）。可以使用梯度下降（Gradient Descent）算法来寻找出范围内最好的函数。

12. 机器不仅可以分辨一张图片里边是不是猫，还可以找出这样选择的理由。

13. 机器还可以在做动物分辨之前，确定这张图片是不是要分类的动物。就是要让机器知道他自己不知道这张图片是啥。

14. 通常来说训练资料和测试资料有同样的分布，即长得差不多。这样往往可以得到很高的正确率。如果测试资料发生了较大的变阿虎，则准确率往往很差，迁移学习可以用来解决这类问题。

15. 机器还可以被训练来产生机器学习的算法。meta-learning。

16. 机器勤奋不懈却又天资不佳。

17. 回归分析的例子

18. ![image-20200803195407922](机器学习.assets/image-20200803195407922.png)

19. 线性模型中，x为输入的特征，w为权重，b为偏差。$y=b+\Sigma w_ix_i$，其中$x_i$为x的一个属性,$w_i$为权重，$b$为偏差。

20. 上标1，2……表示数据编号。

21. ![image-20200805183255117](机器学习.assets/image-20200805183255117.png)

22. 损失函数（loss function）L，的输入是一个一个的函数，输出是数值，表示这个函数的好坏。在线性模型中，函数有两个参数确定，b和w。因此L也可以看做是b和w的函数。L实际是在衡量这一对参数b,w的好坏。

23. ![image-20200805183738379](机器学习.assets/image-20200805183738379.png)

24. 真实值-预测值，再平方。

25. 最后就是寻找二元函数L的最小值点，即w和b的取值。

26. 梯度下降算法可以用来求解极值点：每次移动的距离和该点的梯度还有学习速率有关。最终会找到一个局部最优点。

27. ![image-20200805184835522](机器学习.assets/image-20200805184835522.png)

28. 对于线性回归问题来说，局部最优点就是全局最优点。因为线性回归问题的损失函数是凸convex的。

29. ![image-20200805184929688](机器学习.assets/image-20200805184929688.png)

30. 对于多参数优化问题，每个参数是独立进行的。

31. ![image-20200805185121513](机器学习.assets/image-20200805185121513.png)

32. 梯度方向（2维）是和等高线（在其平面内）垂直的。

33. ![image-20200805185309813](机器学习.assets/image-20200805185309813.png)

34. 可以通过提高模型的复杂程度来减小训练集的误差（每个点的真实值-预测值，再求和）。但是可能会出现过拟合，导致测试集的误差增大overfitting。

35. 以多项式拟合为例，高阶模型是包含低阶模型的所有函数的。高阶模型总是能够找到比低阶模型更符合训练集的函数。

36. ![image-20200814172607469](机器学习.assets/image-20200814172607469.png)

37. 复杂的模型不总是能够在测试集上有更好的表现，这个现象叫做过拟合，overfitting。

38. 对于宝可梦升级前后战斗力的预测公式，不应仅考虑升级前的战斗力，还应考虑不同的宝可梦种类，不同种的宝可梦使用不同的函数。这里可以使用δ函数来实现。使得拟合函数变成多元线性函数。

39. ![image-20200814181238704](机器学习.assets/image-20200814181238704.png)

40. 如果直接使用高阶函数来拟合，可能会产生过拟合的现象，但是正则化loss函数之后就会好很多。

41. 正则化（Regularization），重新定义了loss 函数，原来的loss 函数主要是观察真实值和预测值的误差error。新的loss函数还增加了对参数的考虑。
    $$
    L=\Sigma_n(\hat{y}^n-(b+\Sigma w_ix_i))^2+\lambda\Sigma(w_i)^2
    $$

42. loss函数越小，拟合越好，同时wi越小，拟合也是越好。因为wi接近0时，表示输入的变化对于输出影响不大。函数比较平滑。对参数不敏感。

43. λ越大，在训练集上的误差就越大。因为此时考虑wi更多一些了。但是此时在测试集上的得到的误差可能会变小。

44. ![image-20200814190419784](机器学习.assets/image-20200814190419784.png)

45. 一般倾向于得到平滑的函数（可以较好地忽略误差），但是有不希望得到太平滑的，例如水平线，因为这样就反应不出来输入的变化对输出的影响了。

46. 通过调整λ，获得在一个较小的误差。上图中可以看出，大概在λ=100时比较合理。

47. 正则化时，不需要考虑bias这一项，因为平移函数不影响它的平滑性。

48. 真实的函数是f^（好像打靶的10环），而预估的函数是f*，二者的误差来源于两个方面，bias和variance。

49. <img src="机器学习.assets/image-20200814191630818.png" alt="image-20200814191630818" style="zoom:50%;" />

50. 低的bias表示枪瞄准的比较好，低的variance表示枪的稳定性比较好。

55. <img src="机器学习.assets/image-20200814202301287.png" alt="image-20200814202301287" style="zoom: 50%;" />